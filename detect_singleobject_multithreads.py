import argparse
import os
import platform
import shutil
import time
from pathlib import Path
import numpy as np

import cv2
import torch
import torch.backends.cudnn as cudnn
from numpy import random

from models.experimental import attempt_load
from utils.datasets import LoadStreams, LoadImages
from utils.general import (
    check_img_size, non_max_suppression, apply_classifier, scale_coords,
    xyxy2xywh,  strip_optimizer, set_logging)
from utils.plots import plot_one_box
from utils.torch_utils import select_device, load_classifier, time_synchronized
from threading import Thread
from queue import Queue
from multiprocessing import Process,Queue

# Image process

###############################################################################


def img_preprocess(img1, img_size=640):
    
    
# Padded resize

    img = letterbox(img1, new_shape=img_size)[0]

    
# Convert

    img = img[:, :, ::-1].transpose(2, 0, 1)  
# BGR to RGB, to 3x416x416

    img = np.ascontiguousarray(img)

    
# cv2.imwrite(path + '.letterbox.jpg', 255 * img.transpose((1, 2, 0))[:, :, ::-1])  # save letterbox image

    return img, img1

# Get target size

###############################################################################


def letterbox(img, new_shape=(640, 640), color=(114, 114, 114), auto=True, scaleFill=False, scaleup=True):
    
# Resize image to a 32-pixel-multiple rectangle https://github.com/ultralytics/yolov3/issues/232

    shape = img.shape[:2]  
# current shape [height, width]

    if isinstance(new_shape, int):
        new_shape = (new_shape, new_shape)

    
# Scale ratio (new / old)

    r = min(new_shape[0] / shape[0], new_shape[1] / shape[1])
    if not scaleup:  
# only scale down, do not scale up (for better test mAP)

        r = min(r, 1.0)

    
# Compute padding

    ratio = r, r  
# width, height ratios

    new_unpad = int(round(shape[1] * r)), int(round(shape[0] * r))
    dw, dh = new_shape[1] - new_unpad[0], new_shape[0] - new_unpad[1]  
# wh padding

    if auto:  
# minimum rectangle

        dw, dh = np.mod(dw, 64), np.mod(dh, 64)  
# wh padding

    elif scaleFill:  
# stretch

        dw, dh = 0.0, 0.0
        new_unpad = (new_shape[1], new_shape[0])
        ratio = new_shape[1] / shape[1], new_shape[0] / shape[0]  
# width, height ratios


    dw /= 2  
# divide padding into 2 sides

    dh /= 2

    if shape[::-1] != new_unpad:  
# resize

        img = cv2.resize(img, new_unpad, interpolation=cv2.INTER_LINEAR)
    top, bottom = int(round(dh - 0.1)), int(round(dh + 0.1))
    left, right = int(round(dw - 0.1)), int(round(dw + 0.1))
    img = cv2.copyMakeBorder(img, top, bottom, left, right, cv2.BORDER_CONSTANT, value=color)  
# add border

    return img, ratio, (dw, dh)


def print_div(text):

    print(text, '\n')
    print('='*40,'\n')
###############################################################################
def multithreading(video,device,half,names,colors,model, q):
    frame = video
    t0 = time.time()
        


    fps_t1 = time.time()

    img, img0 = img_preprocess(frame)   
# img: Resize , img0:Orginal
    img = torch.from_numpy(img).to(device)
    img = img.half() if half else img.float()  
# uint8 to fp16/32

    img /= 255.0  
# 0 - 255 to 0.0 - 1.0

    if img.ndimension() == 3:
        img = img.unsqueeze(0)

        
# Inference

    t1 = time_synchronized()
    pred = model(img, augment=opt.augment)[0]

        
# Apply NMS : 取得每項預測的數值

    pred = non_max_suppression(pred, opt.conf_thres, opt.iou_thres, classes=opt.classes, agnostic=opt.agnostic_nms)
    t2 = time_synchronized()

        
# Apply Classifier : 取得該數值的LAbel

    if False:
        pred = apply_classifier(pred, modelc, img, img0)
        
        
# Draw Box

    for i, det in enumerate(pred):

        s = '%gx%g ' % img.shape[2:]  
# print string

        gn = torch.tensor(img0.shape)[[1, 0, 1, 0]]  
# normalization gain whwh

        if det is not None and len(det):
                
# Rescale boxes from img_size to im0 size

            det[:, :4] = scale_coords(img.shape[2:], det[:, :4], img0.shape).round()

                
# Print results

            for c in det[:, -1].unique():
                n = (det[:, -1] == c).sum()  
# detections per class

                s += '%g %ss, ' % (n, names[int(c)])  
# add to string


                
# Write results

            for *xyxy, conf, cls in reversed(det):

                label = '%s %.2f' % (names[int(cls)], conf)
                plot_one_box(xyxy, img0, label=label, color=colors[int(cls)], line_thickness=3)

        
# Print Results(inference + NMS)

    print('%sDone. (%.3fs)' % (s, t2 - t1))

        
# Draw Image

    x, y, w, h = (img0.shape[1]//4), 25, (img0.shape[1]//2), 30
    cv2.rectangle(img0, (x, 10),(x+w, y+h), (0,0,0), -1)
        

    

    cv2.putText(img0, '{} | inference: {:.4f}s | fps: {:.4f}'.format(opt.weights[0], t2-t1, 1/(time.time()-t0)),(x+20, y+20),cv2.FONT_HERSHEY_SIMPLEX,1,(0,0,255),2)

    q.put(img0)
def detect(save_img=False):
    print_div('INTIL')
    out, source, weights, view_img, save_txt, imgsz,count = \
        opt.output, opt.source, opt.weights, opt.view_img, opt.save_txt, opt.img_size,opt.threads
  
# Initialize

    print_div('GET DEVICE')
    set_logging()
    device = select_device(opt.device)
    half = device.type != 'cpu'  
# half precision only supported on CUDA
   
# Load model
    print_div('LOAD MODEL')
    model = attempt_load(weights, map_location=device)  
# load FP32 model

    imgsz = check_img_size(imgsz, s=model.stride.max())  
# check img_size

    if half:
        model.half()  
# to FP16


    
# Second-stage classifier

    print_div('LOAD MODEL_CLASSIFIER')
    classify = False
    if classify:
        modelc = load_classifier(name='resnet101', n=2)  
# initialize

        modelc.load_state_dict(torch.load('weights/resnet101.pt', map_location=device)['model'])  
# load weights

        modelc.to(device).eval()

    
# Get names and colors

    print_div('SET LABEL COLOR')
    names = model.module.names if hasattr(model, 'module') else model.names
    colors = [[random.randint(0, 255) for _ in range(3)] for _ in range(len(names))]

    
# Run inference
###############################################################################
    print_div("RUN INFERENCE")
    img = torch.zeros((1, 3, imgsz, imgsz), device=device)  
# init img
    _ = model(img.half() if half else img) if device.type != 'cpu' else None  
# run muti

    video_path = source
    cap = cv2.VideoCapture(video_path)
    threads = []
    q = Queue()
    rescale = 0.5
    time_count=time.time()
    while cap.isOpened():
        #img0, re_img0=multithreading(cap.read(),device,half,names,colors,model)
        key = cv2.waitKey(1)
        if key == ord('q'): return
            
        for i in range(1):
            ret, frame = cap.read()
            if not ret: 
                print_div('No Frame')
                break;
            threads.append(Thread(target = multithreading, args = (frame,device,half,names,colors,model, q)))
            threads[i].start()
            img0=q.get()
            re_img0 = (int(img0.shape[1]*rescale) ,int(img0.shape[0]*rescale))
            cv2.imshow('Stream_Detected', cv2.resize(img0, re_img0) )
            
        for i in range(1):
            if not ret: 
                print_div('No Frame')
                break;
            threads[i].join()
        threads=[]
        if not ret: 
            print_div('No Frame')
            break;
    # After break
    cap.release()
    cv2.destroyAllWindows()
    print("Spending time: "+str(time.time()-time_count))
if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument('--weights', nargs='+', type=str, default='yolov5s.pt', help='model.pt path(s)')
    parser.add_argument('--source', type=str, default='inference/images', help='source')  
 
# file/folder, 0 for webcam

    parser.add_argument('--output', type=str, default='inference/output', help='output folder')  
# output folder

    parser.add_argument('--img-size', type=int, default=640, help='inference size (pixels)')
    parser.add_argument('--conf-thres', type=float, default=0.4, help='object confidence threshold')
    parser.add_argument('--iou-thres', type=float, default=0.5, help='IOU threshold for NMS')
    parser.add_argument('--device', default='', help='cuda device, i.e. 0 or 0,1,2,3 or cpu')
    parser.add_argument('--view-img', action='store_true', help='display results')
    parser.add_argument('--save-txt', action='store_true', help='save results to *.txt')
    parser.add_argument('--classes', nargs='+', type=int, help='filter by class: --class 0, or --class 0 2 3')
    parser.add_argument('--agnostic-nms', action='store_true', help='class-agnostic NMS')
    parser.add_argument('--augment', action='store_true', help='augmented inference')
    parser.add_argument('--update', action='store_true', help='update all models')
    opt = parser.parse_args()
    print(opt)

    with torch.no_grad():
        if opt.update:  
# update all models (to fix SourceChangeWarning)

            for opt.weights in ['yolov5s.pt', 'yolov5m.pt', 'yolov5l.pt', 'yolov5x.pt']:
                detect()
                strip_optimizer(opt.weights)
        else:
            detect()